{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44012c4",
   "metadata": {},
   "source": [
    "# Captum: Model Interpretability for PyTorch\n",
    "\n",
    "Captum is a model interpretability library for PyTorch that provides insights into how models make predictions. It offers:\n",
    "\n",
    "- Attribution techniques to identify input feature importance\n",
    "- Tools for understanding model behavior and decision-making\n",
    "- Methods to analyze neural network internals\n",
    "- Visualization capabilities for model explanations\n",
    "- Support for both vision and text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ce313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from captum.attr import ShapleyValueSampling, LLMAttribution, TextTemplateInput, ProductBaselines\n",
    "from captum.attr import IntegratedGradients\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c0488",
   "metadata": {},
   "source": [
    "# Model and Tokenizer\n",
    "We selected the DistilGPT model and tokenizer from Huggingface, since it is\n",
    "- reasonably small (parameters are ca 350MB)\n",
    "- runs relatively fast, even on CPU\n",
    "- funfact: biases (e.g. gender) are more pronounced than in ChatGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294884ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3672a0",
   "metadata": {},
   "source": [
    "Wrap model in various attribution techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs = ShapleyValueSampling(model)\n",
    "ingrad = IntegratedGradients(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
