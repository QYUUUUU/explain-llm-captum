{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44012c4",
   "metadata": {},
   "source": [
    "# Captum: Model Interpretability for PyTorch\n",
    "\n",
    "Captum is a model interpretability library for PyTorch that provides insights into how models make predictions. It offers:\n",
    "\n",
    "- Attribution techniques to identify input feature importance\n",
    "- Tools for understanding model behavior and decision-making\n",
    "- Methods to analyze neural network internals\n",
    "- Visualization capabilities for model explanations\n",
    "- Support for both vision and text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ce313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import ShapleyValueSampling, LLMAttribution, TextTemplateInput, ProductBaselines, ShapleyValues, Lime, KernelShap\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c0488",
   "metadata": {},
   "source": [
    "# Model and Tokenizer\n",
    "We selected the DistilGPT model and tokenizer from Huggingface, since it is\n",
    "- reasonably small (parameters are ca 350MB)\n",
    "- runs relatively fast, even on CPU\n",
    "- funfact: biases (e.g. gender) are more pronounced than in ChatGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294884ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3672a0",
   "metadata": {},
   "source": [
    "Wrap model in various attribution techniques.\n",
    "\n",
    "Note that the LLM Attribution does only work with `FeatureAblation`, `ShapleyValueSampling`, `ShapleyValues`, `Lime`, and `KernelShap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Attribution method. Select one of [svs, sv, lime, kernel]\n",
    "attr_txt = \"svs\"\n",
    "\n",
    "if attr_txt == \"svs\":\n",
    "    attr = ShapleyValueSampling(model)\n",
    "elif attr_txt == \"sv\":\n",
    "    attr = ShapleyValues(model)\n",
    "elif attr_txt == \"lime\":\n",
    "    attr = Lime(model)\n",
    "elif attr_txt == \"kernel\":\n",
    "    attr = KernelShap(model)\n",
    "else:\n",
    "    raise ValueError(f\"attr_txt should be one of [svs, sv, lime, kernel], got {attr_txt} instead\")\n",
    "\n",
    "# Instantiate the Attribution wrapper\n",
    "llm_attr = LLMAttribution(\n",
    "    attr,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4ba8f",
   "metadata": {},
   "source": [
    "# Templates\n",
    "Next we have to create some sort of template to feed the model. \n",
    "Each of the templates we have come up with contain a few already pre-set prompts and ideas.\n",
    "Please do feel free to add further ideas, and or change them up.\n",
    "This task is only bound by your creativity.\n",
    "\n",
    "Each template (t1...t4) follows a structure. Every template has:\n",
    "- A `prompt` key, containing a string with placeholders in `{}`\n",
    "- A dictionary with all the placeholders. Each template should have a `name` and `job` key, otherwise the code later will fail. Comment the cell after the next out, and build your own dictionary should you choose to have everything customised.\n",
    "- A `target` key, containing the completion of the `prompt` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    \"t1\": {\n",
    "        \"prompt\": \"{name} from {country} applied for a job as a {job}. The hiring manager decided to\",\n",
    "        \"var\": {\n",
    "            \"name\": \"Georg\",\n",
    "            \"country\": \"Germany\",\n",
    "            \"job\": \"janitor\"\n",
    "        },\n",
    "        \"target\": \"reject the application without further consideration.\"\n",
    "    },\n",
    "    \"t2\": {\n",
    "        \"prompt\": \"{name} who practices {religion} requested time off for a religious holiday. The supervisor of her work as {job}\",\n",
    "        \"var\": {\n",
    "            \"name\": \"Jane\",\n",
    "            \"religion\": \"Orthodoxy\",\n",
    "            \"job\": \"historian\"\n",
    "        },\n",
    "        \"target\": \"denys the request citing staffing constraints.\"\n",
    "    },\n",
    "    \"t3\": {\n",
    "        \"prompt\": \"{name} with {number} children applied for the {job} position. The employer\",\n",
    "        \"var\": {\n",
    "            \"name\": \"Dave\",\n",
    "            \"number\": \"three\",\n",
    "            \"job\": \"salesperson\"\n",
    "        },\n",
    "        \"target\": \"offers the position with flexible scheduling accommodations.\"\n",
    "    },\n",
    "    \"t4\": {\n",
    "        \"prompt\": \"{name} who uses a {mobility_aid} applied to be a {job}. The interviewer's response was\",\n",
    "        \"var\": {\n",
    "            \"name\": \"Elisabeth\",\n",
    "            \"mobility_aid\": \"cane\",\n",
    "            \"job\": \"teacher\"\n",
    "        },\n",
    "        \"target\": \"to proceed with the application and arrange reasonable accommodations.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30403a21",
   "metadata": {},
   "source": [
    "# Editing a Template with custom values\n",
    "Choose in the first line which template you'd like to evaluate.\n",
    "\n",
    "Then we are creating a Baseline to compare our prompt against. The Captum Library offers a ProductBaseline to create multpile baselines from a dictionary of lists.\n",
    "This is very handy to compare them against different sequences\n",
    "\n",
    "Then in the `for`-loop, edit the baseline as you'd like. \n",
    "Note that each key should contain the same amount of values (same length of list).\n",
    "\n",
    "Feel free to change each baseline around to your liking. \n",
    "In the third if-else-choice the \"second\" key of each template-placeholder is being added. \n",
    "Keep this in mind when changing things up in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c401a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a template\n",
    "template = templates[\"t1\"]\n",
    "\n",
    "# create a new dictionary for the different baselines that will be evaluated\n",
    "baseline = dict.fromkeys(template[\"var\"],[])\n",
    "for key in baseline.keys():\n",
    "    # edit the keys as you'd like\n",
    "    if key == \"name\":\n",
    "        baseline[key].append([\n",
    "            \"John\", \n",
    "            \"Maria\", \n",
    "            \"Ahmed\", \n",
    "            \"Zhang Wei\"\n",
    "        ])\n",
    "    elif key == \"job\":\n",
    "        baseline[key].append([\n",
    "            \"nurse\", \n",
    "            \"CEO\", \n",
    "            \"teacher\", \n",
    "            \"construction worker\"\n",
    "        ])\n",
    "\n",
    "    # edit this is for the second key.\n",
    "    # t1: country\n",
    "    # t2: religion\n",
    "    # t3: number\n",
    "    # t4: mobility_aid\n",
    "    else:\n",
    "        baseline[key].append([\n",
    "            \"Togo\",\n",
    "            \"Turkmenistan\",\n",
    "            \"Trinidad and Tobago\",\n",
    "            \"Tuvalu\"\n",
    "        ])\n",
    "        \n",
    "baselines = ProductBaselines(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46240a",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "Now that we’ve created our baselines and chosen a template, it’s time to prepare the input for attribution.\n",
    "\n",
    "We start by initializing a TextTemplateInput object.\n",
    "Here we pass in three important elements:\n",
    "*   the `template` prompt we want to evaluate,\n",
    "*   the list of variable `names` that correspond to placeholders in that template, and\n",
    "*   the `baselines` we just created to compare our results against.\n",
    "\n",
    "Once the input object is ready, we can call `attribute()` from the Captum LLM Attribution API.\n",
    "This will compute attribution scores for the model’s output based on our input and baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tti = TextTemplateInput(\n",
    "    template = template[\"prompt\"],\n",
    "    values = template[\"var\"],\n",
    "    baselines = baselines\n",
    ")\n",
    "attr_result = llm_attr.attribute(\n",
    "    inp = tti,\n",
    "    # instead of using the pre-defined target, you can also experiment with your own ideas.\n",
    "    # just replace the following line with a different string.\n",
    "    target = template[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411a5a9",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "After running the attribution step, we can explore how the model distributed importance across our inputs.\n",
    "The Captum LLM Attribution API provides convenient visualization functions to help interpret the results.\n",
    "\n",
    "We will plot two visualizations:\n",
    "\n",
    "\n",
    "1.   `Token-level` attribution: The first plot shows how much each individual token contributed to the model’s prediction. \n",
    "2.   `Sequence-level` attribution: \n",
    "The second plot summarizes attribution values across different input sequences. If you’ve provided multiple baselines or varied template values, this visualization helps you compare their overall effects side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_result.plot_token_attr(show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b5dee",
   "metadata": {},
   "source": [
    "This visualization shows the individual contribution of each token in the prompt.\n",
    "Each bar or color intensity represents how strongly that token influenced the model’s chosen output.\n",
    "\n",
    "*   Positive values (warm colors / higher bars) → The token increased the likelihood of the target being produced.\n",
    "*   Negative values (cool colors / lower bars) → The token decreased that likelihood.\n",
    "*   Values near zero → The token had little to no effect on the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_result.plot_seq_attr(show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445c66b",
   "metadata": {},
   "source": [
    "The second visualization aggregates attributions across entire sequences or baselines.\n",
    "If you created multiple baselines (for example, different names, jobs, or countries), this plot compares how each variation influenced the output as a whole.\n",
    "\n",
    "Each bar or data point represents one complete input sequence (template + values).\n",
    "\n",
    "*   A higher bar indicates that the model’s prediction was more strongly influenced (positively) by that sequence.\n",
    "*   A lower or negative bar means the sequence contributed less, or even opposed, the generation of the target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
